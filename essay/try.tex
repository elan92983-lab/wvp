\documentclass[11pt, a4paper]{article}

% --- 基础宏包 ---
\usepackage[UTF8]{ctex}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{abstract}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{siunitx}

% --- 样式设置 ---
\setlist[itemize]{label=-}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}

% --- 代码与命令行展示 ---
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    columns=fullflexible,
    frame=single,
    rulecolor=\color{black!20},
    backgroundcolor=\color{black!2},
}

% --- 论文元数据 ---
\title{\Large \textbf{基于 Transformer 架构与谱嵌入的反馈诱导量子优化算法参数零次推理：理论、架构与物理泛化分析}}
\author{潘立扬}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
反馈诱导量子优化算法（FALQON）通过李雅普诺夫反馈律绕开了变分参数搜索，但每一层仍需评估期望值，导致显著的测量/采样开销。本文结合项目代码实现，构建了一个“教师--学生”零次推理框架：教师端用经典仿真执行 FALQON 产生参数轨迹 $\{\beta_p\}_{p=1}^{P}$ 与能量序列；学生端用神经模型对图结构（MaxCut）直接回归整段 $\beta$ 序列，并将其代入同一演化算符得到最终能量，从而以一次前向推理替代层层反馈测量。

我们实现并对比了两类学生模型：基于序列 Transformer 的全局建模（项目主模型），以及一个不依赖外部图库的轻量 GNN 基线（局部消息传递）。评估指标采用近似比 $\mathrm{AR}$，以经典 FALQON 结果为基准统计平均值与标准差，并给出可复现的 Slurm 数组任务流水线（生成、合并、评估、汇总）。在一个 12 节点外推测试样本集上，我们观察到 Avg AR $\approx 1.0031$ 且标准差显著降低（示例统计见表格），提示在某些随机图分布下，谱统计的稳定性可能帮助模型获得跨规模稳健性。
\end{abstract}

\section{引言}
在嘈杂中型量子（NISQ）时代，如何高效获取变分量子算法（VQA）的控制参数是学术界的研究热点。传统 QAOA 依赖经典优化器在多维非凸景观中搜索能量极小值，易受“贫瘠高原”、梯度估计方差等问题影响。FALQON 通过确定性反馈律在一定程度上绕开了外部优化器，但反馈律本身需要反复估计对易子期望值，造成测量瓶颈。

本项目的核心问题可表述为：给定 MaxCut 图实例 $G=(V,E)$，是否可以直接学习从图结构到 FALQON 参数轨迹的映射
\[
\mathcal{F}:\; A(G)\mapsto (\beta_1,\ldots,\beta_P),\quad P=30,
\]
并在推理时用预测的 $\beta$ 序列重放同一演化过程，从而得到接近教师端的切割质量（Cut）。

\section{理论推导与算法框架}

\subsection{反馈动力学的李雅普诺夫导出}
考虑由问题哈密顿量 $H_p$ 和驱动哈密顿量 $H_d$ 描述的系统。目标是最小化成本函数 $C(t)=\langle\psi(t)|H_p|\psi(t)\rangle$。根据薛定谔方程，其对时间的导数为：
\begin{equation}
\frac{d C(t)}{dt} = i \beta(t) \langle \psi(t) | [H_d, H_p] | \psi(t) \rangle.
\end{equation}
为了确保成本函数随时间单调递减（即 $dC/dt\le 0$），构造反馈控制律：
\begin{equation}
\beta(t) = -\alpha\,\langle \psi(t) | i[H_d, H_p] | \psi(t) \rangle.
\end{equation}
在 FALQON 的离散化实现中，状态演化算符为 $U_p=e^{-i\beta_p H_d}e^{-iH_p\Delta t}$。

\paragraph{与实现的一致性} 项目实现中首先预计算 $A=i(H_d H_p - H_p H_d)$（见算法文件），并在每层更新
\[
\beta_p = -\alpha\,\langle \psi_p|A|\psi_p\rangle,\quad \psi_{p+1}=e^{-i\beta_p H_d}e^{-iH_p}\,\psi_p.
\]
这使得我们可以把“反馈生成的 $\beta$ 序列”看作一个监督信号，用于训练学生模型回归整段轨迹。

\subsection{图结构的哈密顿量编码}
对于 $n$ 节点的 MaxCut 问题，问题哈密顿量定义为：
\begin{equation}
H_p = \sum_{(i,j) \in E} w_{ij} \frac{I - Z_i Z_j}{2}.
\end{equation}
由于 $H_p$ 完全由图的邻接矩阵 $A$ 决定，参数轨迹 $\vec{\beta}$ 是矩阵 $A$ 的非线性函数。本文假设存在映射 $\mathcal{F}:A\mapsto\vec{\beta}$，并利用神经网络进行逼近。

\paragraph{Cut 与能量的换算} 在代码评估中，我们以能量期望 $E=\langle H_p\rangle$ 换算切割值：
\begin{equation}
\mathrm{Cut} = \frac{|E(G)| - 2E}{2},\qquad \mathrm{AR}=\frac{\mathrm{Cut}_{\mathrm{AI}}}{\mathrm{Cut}_{\mathrm{FALQON}}}.
\end{equation}

\section{数据集构建与可复现实验流水线}

\subsection{教师数据生成：并行分片与合并}
数据由“随机图 + 教师 FALQON 轨迹”组成，每个样本包含：节点数、邻接矩阵 $A$、$P=30$ 层的 $\beta$ 序列与能量序列。生成脚本支持 Slurm Job Array 分片：每个数组任务生成一段索引范围并写入 \texttt{part\_k.npz}。

典型调用方式如下（与项目脚本一致）：
\begin{lstlisting}
python -u scripts/generate_dataset_v2.py --start 0 --end 50 --part_id 0
\end{lstlisting}

所有分片最终通过合并脚本汇总为一个压缩文件（\texttt{train\_data\_final.npz}）。

\subsection{数据格式与 Padding}
训练与评估统一使用最大节点数 $N_{\max}=12$ 进行 padding：将不同规模的邻接矩阵填充到 $12\times 12$，并使用 mask 指示真实节点位置。该处理与训练入口保持一致，从而避免推理时输入分布不匹配。

\section{模型实现与训练细节}

\subsection{Transformer 学生：全局建模}
项目主模型将展平后的邻接矩阵视为输入特征，通过线性层映射到 $d_{\text{model}}=64$ 的嵌入空间，并用多层 Transformer 解码器回归长度 $P=30$ 的 $\beta$ 序列。尽管代码中将“图特征”压缩为单 token 进行解码，这仍可视为一种全局表示学习：模型用注意力机制对整图统计进行建模，输出整段轨迹。

\subsection{GNN 基线：局部消息传递}
为提供对照，我们实现了一个不依赖外部图库的轻量 GNN：输入为 padding 后的邻接矩阵与 mask；加入 self-loop 并做 $\tilde{A}=D^{-1/2}(A+I)D^{-1/2}$；消息传递为 $H^{(\ell+1)}=\sigma(\tilde{A}H^{(\ell)}W^{(\ell)})$；读出采用 mask 加权的 mean pooling + MLP 输出 $\beta$ 序列。

\section{并行评估：Transformer vs. GNN vs. Classical}

\subsection{评估流程}
评估脚本读取 \texttt{train\_data\_final.npz} 中的样本，分别对 Transformer 与 GNN 前向得到 $\beta$ 序列，并在同一 $H_p,H_d$ 下重放演化得到 $\mathrm{Cut}_{\mathrm{AI}}$；再用样本内保存的教师能量（或缺失时重新跑教师）得到 $\mathrm{Cut}_{\mathrm{FALQON}}$，计算 $\mathrm{AR}$。

\subsection{Slurm 数组并行与结果汇总}
在 CPU 节点上，评估采用数组任务把样本区间分成若干块，每块内部使用多进程池并行计算，并将每块结果保存为 \texttt{output/ar\_parts/part\_\{start\}.npy}（GNN 额外保存 \texttt{part\_\{start\}\_gnn.npy}）。

最终用合并脚本计算全局均值与标准差：
\begin{lstlisting}
python3 scripts/merge_ar_parts.py --parts_dir output/ar_parts --kind transformer
python3 scripts/merge_ar_parts.py --parts_dir output/ar_parts --kind gnn
\end{lstlisting}

\subsection{实验统计结果与讨论}
表 \ref{tab:dataset_results} 汇总了当前项目流水线在训练/评估数据集上的统计口径。需要强调：由于训练数据分布为 4--10 节点随机图，若要讨论 12/20 节点外推，应另行生成对应规模的数据集并重复相同评估流程。

\begin{table}[htbp]
\centering
\caption{训练/评估数据集（4--10 节点随机图）上的 AR 统计结果}
\label{tab:dataset_results}
\begin{tabular}{lcccc}
\toprule
测试场景 & 样本数 & 平均近似比 (Avg AR) & 标准差 (Std) & 备注 \\
\midrule
训练/评估数据集 (4--10 节点) & 744 & 1.0132 & 0.4834 & Transformer Aggregated \\
GNN (Baseline) & 5 & 0.9957 & 0.0141 & 仅含部分分片示例 \\
\bottomrule
\end{tabular}
\end{table}

\section{Transformer 模型架构与创新点}

\subsection{输入特征嵌入与谱位置编码}
模型将展平后的邻接矩阵作为 Token 输入。为使模型感知拓扑，我们引入了归一化拉普拉斯算子 $L = I - D^{-1/2} A D^{-1/2}$ 的特征分解，提取其特征向量作为谱位置编码（Spectral Positional Encoding）。这使得模型能够直接学习与哈密顿量谱统计相关的结构特征。

\subsection{编码器结构与注意力机制}
核心组件为多头注意力机制（Multi-Head Attention）：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V.
\end{equation}
\textbf{物理直觉：}全局注意力机制允许模型同时分析图中的局部簇与长程连通性。

\subsection{预测头与输出}
编码后的图级表示可通过池化/读出汇聚为向量，并经由 MLP 直接回归长度 $P=30$ 的连续参数序列。该“整段轨迹回归”对应一次前向推理输出全层参数，从而在推理阶段避免逐层反馈测量。

\subsection{方法论分析：为何 Transfomer 优于 GNN 捕捉宏观谱统计}
图神经网络（GNN）通常基于消息传递机制（MPNN），其 $L$ 层聚合仅能覆盖 $L$-跳邻域。若要捕捉决定 FALQON 演化轨迹的全局谱特征（如 $H_p$ 的高阶矩 $\text{Tr}(A^k)$ 或特征值分布），MPNN 需要堆叠至图直径深度，这常导致过平滑（Over-smoothing）问题，使得节点表示趋同而丢失结构信息：
\begin{equation}
\lim_{L \to \infty} \mathbf{H}^{(L)} \approx \mathbf{1} \mathbf{c}^T.
\end{equation}
相比之下，Sequence Transformer 具有全局感受野（Global Receptive Field）。通过自注意力机制：
\begin{equation}
\text{Attn}(\mathbf{X})_i = \sum_{j=1}^N \frac{\exp(\mathbf{x}_i^T \mathbf{x}_j / \sqrt{d_k})}{\sum_{k=1}^N \exp(\mathbf{x}_i^T \mathbf{x}_k / \sqrt{d_k})} \mathbf{v}_j,
\end{equation}
任意节点对 $(i, j)$ 无论拓扑距离多远，其交互路径长度均为 1。这种完全图式的连接使其能直接聚合全图信息来逼近全局谱统计量（Global Spectral Statistics），从而更准确地建立 $A(G) \to \vec{\beta}$ 的非线性映射，避免了局部算子在重构长程关联时的指数级衰减。

\section{物理分析：泛化能力与跨规模稳健性}

\subsection{谱密度与泛化基础}
从物理直觉上，FALQON 的反馈律由对易子期望值驱动，而对易子的统计行为与 $H_p$（由图结构决定）的谱性质密切相关。对于 Erd\H{o}s--R\'{e}nyi 随机图，随着规模增大，其邻接/拉普拉斯谱分布在统计意义上趋于稳定；因此我们提出一种可检验的假说：\emph{当谱统计在不同规模间保持相近时，学生模型更可能实现跨规模泛化}。

\subsection{近似比分析与 12 节点外推示例}
在一个 12 节点外推测试样本集（示例：100 个样本）上，我们观察到 Transformer 的 Avg AR 为 1.0031，Std 为 0.0376（表 \ref{tab:extrap_12_results}）。需要强调：外推结论需要在固定规模的独立测试集上按同一口径复现实验。

\begin{table}[htbp]
\centering
\caption{示例：Transformer 在训练域内与 12 节点外推测试上的 AR 统计（含 Min/Max）}
\label{tab:extrap_12_results}
\begin{tabular}{lccccc}
\toprule
测试场景 & 样本数 & 平均近似比 (Avg AR) & 标准差 (Std) & 最小值 (Min) & 最大值 (Max) \\
\midrule
训练域内 (4--10 节点) & 100 & 1.0009 & 0.1411 & 0.6147 & 1.4577 \\
外推测试 (12 节点) & 100 & \textbf{1.0031} & \textbf{0.0376} & \textbf{0.8181} & \textbf{1.1080} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Transformer vs. GNN 对比（同口径 AR，分片示例）}
为了给出与局部消息传递基线的直接对照，我们在同一评估口径下统计了 Transformer、GNN 与经典 FALQON 的近似比（AR）。在一次评估分片上得到的统计量为：Transformer 的 Avg AR 为 1.025287、Std 为 0.070539；GNN 的 Avg AR 为 0.995718、Std 为 0.014080；经典 FALQON 作为基准按定义为 1.0。

\begin{table}[htbp]
\centering
\caption{Transformer / GNN / Classical 的近似比（AR）对比（单次评估分片统计）}
\label{tab:ar_compare_part}
\begin{tabular}{lcc}
\toprule
方法 & 平均近似比 (Avg AR) & 标准差 (Std) \\
\midrule
Transformer & 1.025287 & 0.070539 \\
GNN (Baseline) & 0.995718 & 0.014080 \\
Classical FALQON & 1.000000 & 0.000000 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{备注：}上述为“分片”统计结果；若使用数组任务跑完整测试集，应对所有分片合并后再报告全局均值与方差。

\subsection{大规模随机正则图上的预测曲线聚类现象}
为了在 $N>20$ 的规模上验证“预测参数曲线的聚类（common profile）”现象，我们在随机 $d$-正则图分布上进行零次推理测试，并仅分析学生模型输出的 $\beta$ 序列形状统计（此处不再运行经典 Teacher 的全量 statevector 反馈仿真，以避免 $N$ 增大带来的指数资源开销）。

实验设置为：固定节点数 $N=24$、度数 $d=3$，随机采样 3-正则图，对每个图预测长度 $P=30$ 的参数序列 $\vec{\beta}$。随后对这些曲线做层次聚类，并用 PCA 将曲线嵌入到二维平面观察聚类结构。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{picture/regular_clustering_curves_zoom.png}
    \caption{随机 3-正则图（$N=24$）上预测的 $\beta$ 曲线聚类结果。图中展示各簇均值曲线（为突出细微差异，绘图时对早期极端尖峰步骤做了缩放/截取）。可见在固定分布下，预测曲线会集中到少数几类“共同轮廓”。}
    \label{fig:regular_curve_cluster}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{picture/regular_clustering_pca.png}
    \caption{对预测的 $\beta$ 曲线做 PCA 降维后的散点图（颜色表示聚类标签）。聚类在低维嵌入空间中依然可分，说明曲线差异具有低维结构。}
    \label{fig:regular_curve_pca}
\end{figure}

\paragraph{理论洞察：Kesten-McKay 定律与轨迹普适性}
参数曲线出现聚类（Common Profile）现象的根本原因在于图谱统计的收敛性。FALQON 的每步反馈值 $\beta_p \propto \langle [H_d, H_p] \rangle$ 非线性地依赖于哈密顿量 $H_p$ 的各阶谱矩（Spectral Moments） $\mu_k = \frac{1}{N}\text{Tr}(H_p^k) = \int \lambda^k \rho(\lambda) d\lambda$。

根据随机矩阵理论，对于随机 $d$-正则图，当 $N \to \infty$ 时，其邻接矩阵特征值的经验谱密度 $\rho_N(\lambda)$ 依概率弱收敛于 Kesten-McKay 分布（亦称作相关随机游走谱分布）：
\begin{equation}
\rho_{\text{KM}}(\lambda) = \begin{cases} 
\frac{d \sqrt{4(d-1) - \lambda^2}}{2\pi (d^2 - \lambda^2)} & |\lambda| \le 2\sqrt{d-1}, \\
0 & \text{otherwise}.
\end{cases}
\label{eq:kesten_mckay}
\end{equation}
这解释了图 \ref{fig:regular_curve_cluster} 中的聚类现象：尽管具体的图实例 $G$ 不同，但它们在热力学极限（$N \gg 1$）下共享相同的极限谱分布 $\rho_{\text{KM}}$。既然控制动力学的 $\beta$ 序列是谱分布的泛函 $\vec{\beta} = \mathcal{G}[\rho(\lambda)]$，那么谱分布的普适性（Universality）必然导致控制轨迹的普适性。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{picture/regular_clustering_spectrum.png}
    \caption{随机 3-正则图（$N=24$）的缩放邻接矩阵谱密度（去除最大特征值后）与 Wigner 半圆律参考曲线的对照。谱密度在 bulk 区域呈稳定形态，为“预测曲线聚类”的随机矩阵解释提供直觉支撑。}
    \label{fig:regular_spectrum_semicircle}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{picture/prediction_result.png}
    \caption{预测的 $\beta$ 参数序列（红虚线）与经典 FALQON 真实序列（蓝实线）的对比轨迹。}
    \label{fig:prediction_result}
\end{figure}

\section{结论}
本文基于真实项目代码给出了一个可复现的“教师--学生”框架：用经典仿真生成 FALQON 轨迹作为监督信号，用神经网络零次推理预测整段参数序列，并通过重放演化得到能量与 Cut 值，从而以近似比 $\mathrm{AR}$ 量化替代反馈测量的效果。后续工作将聚焦于：在固定 $n=12,20$ 的外推数据集上进行同口径对比；将谱统计显式编码为多 token 输入以增强结构可解释性；以及在更贴近 NISQ 的噪声模型下评估“零次推理”对采样开销的真实收益。

\end{document}