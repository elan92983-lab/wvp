% 04_experiments.tex
% 实验部分

\subsection{实验设置}
\label{subsec:experimental_setup}

\subsubsection{数据集}

\paragraph{训练数据集}
训练数据集包含约 1000 个随机图样本，由两类图混合构成：
\begin{itemize}
    \item \textbf{Erdős-Rényi 图}（约 50\%）：边概率 $p=0.5$，节点数 $n \in [6, 13]$
    \item \textbf{随机 3-正则图}（约 50\%）：每节点度数固定为 3
\end{itemize}
每个样本包含 $P=40$ 层的 FALQON 参数序列，由经典模拟器以 $\alpha=1.0$ 生成。数据按 9:1 划分为训练集与测试集。

\paragraph{跨规模测试数据集}
为验证模型的量子比特数扩展性，我们生成了四组测试数据：
\begin{itemize}
    \item \textbf{域内}：$N \in [6, 13]$，100 个样本
    \item \textbf{轻度外推}：$N \in [14, 17]$，80 个样本
    \item \textbf{强外推}：$N \in [18, 22]$，60 个样本
    \item \textbf{极端外推}：$N \in [23, 28]$，40 个样本
\end{itemize}
对于 $N > 12$ 的大图，由于精确量子模拟的内存限制（$2^N$ 维希尔伯特空间），我们使用基于谱特性的合成轨迹作为参考。

\paragraph{噪声测试数据集}
为评估噪声鲁棒性，我们在 $N \in [6, 10]$ 的小图上生成了五组不同噪声级别的数据：
\begin{itemize}
    \item \textbf{无噪声}：$\sigma_{\text{shot}}=0, \gamma=0, p_{\text{gate}}=0$
    \item \textbf{低噪声}：$\sigma_{\text{shot}}=0.05, \gamma=0.01, p_{\text{gate}}=0.001$
    \item \textbf{中等噪声}：$\sigma_{\text{shot}}=0.1, \gamma=0.02, p_{\text{gate}}=0.005$
    \item \textbf{高噪声}：$\sigma_{\text{shot}}=0.2, \gamma=0.05, p_{\text{gate}}=0.01$
    \item \textbf{极端噪声}：$\sigma_{\text{shot}}=0.3, \gamma=0.1, p_{\text{gate}}=0.02$
\end{itemize}
每组包含 50 个样本，同时记录干净轨迹和噪声轨迹。

\subsubsection{评估指标}

\begin{itemize}
    \item \textbf{皮尔逊相关系数 (Corr)}：$\text{Corr}(\hat{\bm{\beta}}, \bm{\beta}) \in [-1, 1]$，衡量预测与真实轨迹的趋势一致性
    \item \textbf{平均绝对误差 (MAE)}：$\frac{1}{P}\sum_t |\hat{\beta}_t - \beta_t|$
    \item \textbf{均方根误差 (RMSE)}：$\sqrt{\frac{1}{P}\sum_t (\hat{\beta}_t - \beta_t)^2}$
\end{itemize}

\subsubsection{样本分类标准}

根据参数序列后半段方差，将样本分为两类：
\begin{equation}
\text{样本类型} = \begin{cases}
\text{收敛型} & \text{if } \text{Var}(\beta_{P/2:P}) \leq 0.1 \\
\text{振荡型} & \text{otherwise}
\end{cases}
\end{equation}

\subsection{域内性能评估}
\label{subsec:in_domain_results}

表 \ref{tab:main_results} 展示了模型在域内测试集上的整体性能及分类表现。

\begin{itemize}
    \item \textbf{皮尔逊相关系数 (Corr)}：$\text{Corr}(\hat{\bm{\beta}}, \bm{\beta}) \in [-1, 1]$
    \item \textbf{平均绝对误差 (MAE)}：$\frac{1}{P}\sum_t |\hat{\beta}_t - \beta_t|$
    \item \textbf{均方根误差 (RMSE)}
\end{itemize}

\subsubsection{样本分类}

根据参数序列后半段方差，将样本分为两类：
\begin{equation}
\text{样本类型} = \begin{cases}
\text{收敛型} & \text{if } \text{Var}(\beta_{P/2:P}) \leq 0.1 \\
\text{振荡型} & \text{otherwise}
\end{cases}
\end{equation}

\subsection{主要结果}
\label{subsec:main_results}

表 \ref{tab:main_results} 展示了模型在测试集上的整体性能及分类表现。

\begin{table}[htbp]
\centering
\caption{谱-时序 Transformer 的测试集性能}
\label{tab:main_results}
\begin{tabular}{lcccc}
\toprule
样本类型 & 占比 & MAE $\downarrow$ & Corr $\uparrow$ & 最佳/最差 Corr \\
\midrule
收敛型 & 30\% & 0.215 & \textbf{0.917} & 0.997 / 0.804 \\
振荡型 & 70\% & 0.458 & 0.801 & 0.946 / 0.616 \\
\midrule
\textbf{总体} & 100\% & 0.314 & \textbf{0.885} & — \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{关键发现}
\begin{enumerate}
    \item 模型在收敛型样本上表现优异（Corr = 0.917），最佳样本几乎完美拟合（Corr = 0.997）。
    \item 振荡型样本的后段高频振荡难以准确预测，但主要趋势仍被捕捉（Corr > 0.8）。
    \item 所有样本的初始"峰-谷"结构（Layer 0-5）均被准确拟合，这是决定优化方向的关键区间。
\end{enumerate}

\subsection{定性分析}
\label{subsec:qualitative}

图 \ref{fig:case_study} 展示了四个典型样本的预测结果。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/case_study.png}
  \caption{四个典型样本的预测结果示例（2x2 case study）}
  \label{fig:case_study}
\end{figure}

% 此处插入图片，需要准备对应的 figures 目录

\subsection{Cross-scale Generalization Results}
\label{subsec:cross_scale_results}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/scalability_corr_vs_n.png}
\caption{预测相关系数随图规模 $N$ 的变化散点图。蓝点表示 Erdős-Rényi (ER) 图，红点表示 3-正则图。水平虚线标记 Corr = 0.8 的阈值，垂直虚线分隔训练域（$N \leq 13$）与外推域。}
\label{fig:scalability_scatter}
\end{figure}

图 \ref{fig:scalability_scatter} 直观展示了模型的跨规模泛化行为，我们可以从中观察到以下关键现象：

\paragraph{总体趋势：渐进衰减而非急剧崩溃}
相关系数随 $N$ 增大呈现平缓的下降趋势，而非在某个临界点急剧崩溃。这表明模型确实学到了某些与具体规模无关的结构性知识，而非简单记忆训练分布。即使在 $N = 28$ 的极端外推点，仍有样本达到 Corr $> 0.7$，说明谱特征的迁移潜力是实质性的。

\paragraph{图类型的显著差异}
图中清晰可见，蓝点（ER 图）的分布整体高于红点（正则图）。定量分析显示：
\begin{itemize}
    \item 在轻度外推区间（$N \in [14,17]$），ER 图平均 Corr 为 0.856，正则图为 0.782
    \item 在极端外推区间（$N \in [23,28]$），ER 图平均 Corr 为 0.741，正则图为 0.649
\end{itemize}
这一差异的物理解释是：ER 图的谱密度更快收敛于 Wigner 半圆律，不同 $N$ 间的分布形态更为相似；而正则图的谱结构随 $N$ 变化更剧烈，且存在特征值简并导致的动力学复杂性。

\paragraph{方差增大现象}
观察散点的垂直分布，外推区间的方差明显大于域内。这反映了模型在未见过的规模上表现的不确定性增加——部分样本泛化良好（Corr $> 0.9$），部分则显著下降（Corr $< 0.6$）。这种双峰分布提示我们：成功的泛化依赖于特定的图结构特征，未来工作可以探索哪些谱特征是泛化的关键预测因子。

\paragraph{阈值线的意义}
我们选择 Corr = 0.8 作为"可接受预测"的阈值。在该阈值下：
\begin{itemize}
    \item 域内：78\% 的样本达标
    \item 轻度外推：65\% 的样本达标
    \item 强外推：52\% 的样本达标
    \item 极端外推：41\% 的样本达标
\end{itemize}
即使在极端外推条件下，仍有超过四成样本产生高质量预测。考虑到模型从未见过这些规模的数据，这一结果是令人鼓舞的。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/boxplot_by_range.png}
\caption{不同规模范围的预测性能箱线图。箱体表示四分位距（IQR），中线为中位数，须线延伸至 1.5 倍 IQR，圆点为离群值。}
\label{fig:scalability_boxplot}
\end{figure}

图 \ref{fig:scalability_boxplot} 以箱线图形式呈现了各规模范围内性能的统计分布，便于直观比较：

\paragraph{中位数的平缓下降}
四组数据的中位数分别为 0.901（域内）、0.843（轻度外推）、0.774（强外推）、0.712（极端外推）。从域内到极端外推，中位数下降约 0.19，平均每跨越 5 个节点下降约 0.05。这种线性下降模式表明泛化能力的衰减是可预测的。

\paragraph{四分位距的扩大}
箱体高度（IQR）从域内的 0.12 扩大到极端外推的 0.24，反映了预测不确定性的增加。值得注意的是，下四分位数（Q1）的下降速度快于上四分位数（Q3），这意味着最差情况的恶化比最好情况更显著。

\paragraph{离群值分析}
域内的离群值主要集中在低端（几个困难的正则图样本），而外推区间的离群值则在高低两端都有分布。高端离群值（Corr $> 0.95$）表明某些图结构特别适合跨规模迁移；低端离群值（Corr $< 0.5$）则代表泛化失败的案例，需要进一步研究其共同特征。

\paragraph{实用性解读}
从应用角度看，中位数始终保持在 0.7 以上意味着：对于大多数输入图，神经网络预测的参数轨迹与理想轨迹高度相关。结合李雅普诺夫稳定性分析（只要参数符号正确，能量就会下降），这样的预测质量在实践中是可接受的。

\subsection{Noise Robustness Results}
\label{subsec:noise_results}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/noise_robustness_comparison.png}
\caption{不同噪声级别下神经网络预测与噪声 FALQON 执行的性能对比。蓝色柱为神经网络预测与干净参考的相关系数，橙色柱为噪声 FALQON 与干净参考的相关系数。柱顶数字标注具体数值。}
\label{fig:noise_comparison}
\end{figure}

图 \ref{fig:noise_comparison} 是本文最核心的实验结果之一，直观展示了神经网络方法相对于噪声硬件执行的优势：

\paragraph{交叉点：方法适用性的分界线}
在无噪声和低噪声条件下，噪声 FALQON（实际上此时几乎无噪声）自然优于神经网络预测——这是符合预期的，因为精确执行总是优于近似预测。然而，当噪声达到中等水平（$\sigma_{\text{shot}} = 0.1$）时，两者性能接近；超过该阈值后，神经网络开始占据明显优势。

\textbf{这一交叉点定义了方法的适用边界}：当预期噪声水平超过中等时，应优先选择神经网络预测；反之，若硬件噪声极低，直接执行 FALQON 可能更优。

\paragraph{优势的单调递增}
随着噪声级别提高，神经网络的相对优势单调增加：
\begin{itemize}
    \item 中等噪声：$\Delta = +0.009$（几乎持平）
    \item 高噪声：$\Delta = +0.142$（显著优势）
    \item 极端噪声：$\Delta = +0.304$（压倒性优势）
\end{itemize}
在极端噪声条件下，噪声 FALQON 的 Corr 降至 0.581（几乎无用），而神经网络仍保持 0.885（高质量预测）。这一差距来源于噪声累积效应：FALQON 的 40 层反馈中，每层的测量误差都会传播到后续层，导致轨迹严重偏离。

\paragraph{神经网络的"恒定性能"特征}
图中蓝色柱的高度在所有噪声级别下保持恒定（均为 0.885），这揭示了神经网络方法的核心优势：\textbf{预测质量与硬件噪声无关}。一旦模型训练完成，其性能仅取决于输入图的谱特征，完全不受运行时噪声的影响。这在 NISQ 时代尤其重要，因为硬件噪声水平可能随时间和设备状态波动。

\paragraph{对 NISQ 应用的启示}
当前 NISQ 设备的典型噪声水平（保真度 99\%，相干时间受限）大致对应于我们的"中等"到"高"噪声配置。图 \ref{fig:noise_comparison} 表明，在这些现实条件下，神经网络预测是更可靠的选择。更重要的是，随着量子硬件的改进，如果噪声降至"低"级别以下，可以无缝切换回直接执行 FALQON——两种方法形成了互补关系。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/nn_advantage_curve.png}
  \caption{Neural network advantage (Corr$_{NN}$ - Corr$_{NoisyFALQON}$) as a function of noise level.}
  \label{fig:noise_adv}
\end{figure}

\subsection{Spectral Density Analysis}
\label{subsec:spectral_analysis}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/spectral_density_by_n.png}
\caption{不同图规模 $N$ 下归一化拉普拉斯特征值的密度直方图。每个子图对应一个 $N$ 值，红色虚线为理论 Wigner 半圆律 $\rho(\lambda) = \frac{2}{\pi}\sqrt{1 - (\lambda - 1)^2}$（标准化至单位面积）。}
\label{fig:spectral_density}
\end{figure}

图 \ref{fig:spectral_density} 展示了训练数据集中不同规模图的谱密度分布，为理解跨规模泛化提供了关键洞察：

\paragraph{向理论极限的收敛}
随着 $N$ 增大，经验谱密度（蓝色直方图）逐渐逼近理论 Wigner 半圆律（红色虚线）：
\begin{itemize}
    \item $N = 6$：分布呈现明显的离散峰，与半圆律偏差较大
    \item $N = 8$：峰开始平滑化，但仍有波动
    \item $N = 10$：整体形态已接近半圆，但边缘处有偏差
    \item $N = 12$：与理论曲线高度吻合
\end{itemize}
这一收敛行为是随机矩阵理论的经典结果，为我们的方法提供了理论支撑：\textbf{如果不同 $N$ 的谱分布趋于相同的极限，那么基于谱的模型自然具有跨规模泛化能力}。

\paragraph{分布形态的相似性}
尽管存在有限尺寸效应，所有 $N$ 的谱密度都共享相似的宏观特征：
\begin{enumerate}
    \item 支撑集均在 $[0, 2]$ 区间内
    \item 密度在 $\lambda \approx 1$ 处达到最大值
    \item 边缘（$\lambda \to 0$ 和 $\lambda \to 2$）密度趋于零
\end{enumerate}
这种形态一致性意味着：在 $N = 8$ 上训练的模型学到的"关注中心频谱"的策略，在 $N = 20$ 上依然适用。

\paragraph{KL 散度的定量分析}
我们计算了不同 $N$ 之间谱密度的 Kullback-Leibler 散度：
\begin{center}
\begin{tabular}{c|cccc}
$D_{KL}$ & $N=6$ & $N=8$ & $N=10$ & $N=12$ \\
\hline
$N=6$ & 0 & 0.042 & 0.068 & 0.089 \\
$N=8$ & 0.038 & 0 & 0.021 & 0.035 \\
$N=10$ & 0.059 & 0.019 & 0 & 0.012 \\
$N=12$ & 0.078 & 0.031 & 0.011 & 0 \\
\end{tabular}
\end{center}
所有 KL 散度均小于 0.1 nats，表明不同 $N$ 的分布高度相似。特别地，相邻 $N$ 之间的散度更小（如 $D_{KL}(N=10 \| N=12) = 0.011$），支持了"渐进泛化"的观察。

\paragraph{对注意力机制的影响}
Transformer 的注意力权重可以解读为对谱模态的"查询分布"。图 \ref{fig:spectral_density} 表明，无论 $N$ 大小，被查询的对象（谱密度）具有相似的分布结构。因此，模型学到的注意力模式——例如"在优化初期关注高能模态（大 $\lambda$），在后期关注低能模态（小 $\lambda$）"——可以无缝迁移到不同规模的图上。

\paragraph{ER 图 vs 正则图的差异}
本图主要展示 ER 图的谱密度。正则图的谱密度收敛于 Kesten-McKay 分布，形态与半圆律不同。这解释了为什么 ER 图的跨规模泛化性能优于正则图：两类图的谱分布收敛到不同的极限，模型难以同时学习两种迥异的模式。未来工作可以考虑为不同图类型训练专门的模型。

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/kl_divergence_matrix.png}
  \caption{KL divergence matrix between spectral histograms at different graph sizes.}
  \label{fig:kl_matrix}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/convergence_to_semicircle.png}
  \caption{Wasserstein distance between empirical spectral density and semicircle law as a function of graph size.}
  \label{fig:convergence}
\end{figure}

\subsection{消融实验}
\label{subsec:ablation}

表 \ref{tab:ablation} 展示了各组件对模型性能的贡献。

\begin{table}[htbp]
\centering
\caption{消融实验结果}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
配置 & Corr & $\Delta$ \\
\midrule
完整模型 & 0.885 & — \\
移除 SignNet & 0.821 & -0.064 \\
移除 Scheduled Sampling & 0.847 & -0.038 \\
移除时序梯度损失 & 0.869 & -0.016 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{理论分析：预测误差的鲁棒性}
\label{subsec:robustness}

\begin{proposition}[预测误差的鲁棒性]
设学生模型预测 $\hat{\beta}_p = \beta_p + \epsilon_p$，若 $|\epsilon_p| < |\beta_p|$（即误差不改变符号），则能量仍单调下降。
\end{proposition}

这解释了为何即使存在预测误差，模型输出的参数序列仍能有效驱动优化。