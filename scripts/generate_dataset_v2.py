import numpy as np
import networkx as nx
import os
import multiprocessing
import argparse
from tqdm import tqdm
from src.algorithms.falqon_core import FALQON

def generate_single_instance(instance_id):
    """
    å•ä¸ªæ ·æœ¬ç”Ÿæˆä»»åŠ¡ï¼šç”Ÿæˆéšæœºå›¾å¹¶è¿è¡Œ FALQON è·å–æ•™å¸ˆç»éªŒæ•°æ®ã€‚
    (é€»è¾‘ä¿æŒä¸å˜)
    """
    try:
        # 1. éšæœºç”ŸæˆèŠ‚ç‚¹æ•° (é’ˆå¯¹ 2026 å¹´åˆæœŸ FTQC ç ”ç©¶ï¼Œå»ºè®®å…ˆä» 4-10 ä¸ªæ¯”ç‰¹å¼€å§‹)
        num_nodes = np.random.randint(4, 11)
        
        # 2. ç”Ÿæˆ ErdÅ‘s-RÃ©nyi éšæœºå›¾ï¼Œæ¦‚ç‡ p ä¸º 0.5 ç¡®ä¿å¤æ‚åº¦
        g = nx.erdos_renyi_graph(num_nodes, p=0.5)
        
        # ç¡®ä¿å›¾æ˜¯è¿é€šçš„ï¼Œå¦åˆ™ MaxCut é—®é¢˜ä¼šé€€åŒ–
        if not nx.is_connected(g):
            return None

        # 3. å®ä¾‹åŒ–æ•™å¸ˆæ¨¡å‹ (FALQON)
        # alpha=0.5 æ˜¯æ–‡çŒ® [3] æ¨èçš„æ­¥é•¿ï¼Œæœ‰åŠ©äºæ•æ‰â€œå³°-å°¾â€ç»“æ„
        falqon = FALQON(g, alpha=0.5)
        
        # 4. è¿è¡Œæ¼”åŒ–ï¼Œè·å– 30 å±‚çš„å‚æ•°åºåˆ—
        # å¢åŠ å±‚æ•°æœ‰åŠ©äº Transformer å­¦ä¹ é•¿åºåˆ—è§„å¾‹
        betas, energies = falqon.train(max_layers=30)
        
        # 5. è¿”å›ç»“æœå­—å…¸
        return {
            "node_count": num_nodes,
            "adj": nx.to_numpy_array(g), # å›¾çš„é‚»æ¥çŸ©é˜µï¼ˆTransformer çš„è¾“å…¥ï¼‰
            "betas": np.array(betas),    # æœ€ä¼˜å‚æ•°æ›²çº¿ï¼ˆTransformer çš„æ ‡ç­¾ï¼‰
            "energies": np.array(energies)
        }
    except Exception as e:
        # åœ¨å¤§è§„æ¨¡å¹¶å‘æ—¶ï¼Œå•ä¸ªé”™è¯¯ä¸åº”ä¸­æ–­æ•´ä¸ªè¿›ç¨‹ï¼Œè¿”å› None å³å¯
        return None

def main():
    # --- ä¿®æ”¹ 1: å¼•å…¥å‚æ•°è§£æï¼Œé€‚é… Job Array ---
    parser = argparse.ArgumentParser(description="FALQON Dataset Generator (Parallel)")
    parser.add_argument("--start", type=int, default=0, help="å½“å‰ä»»åŠ¡çš„èµ·å§‹ç´¢å¼•")
    parser.add_argument("--end", type=int, default=100, help="å½“å‰ä»»åŠ¡çš„ç»“æŸç´¢å¼•")
    parser.add_argument("--part_id", type=int, default=0, help="å½“å‰åˆ†ç‰‡æ–‡ä»¶çš„ç¼–å· (å¯¹åº” Slurm Array ID)")
    args = parser.parse_args()

    # --- ä¿®æ”¹ 2: åŠ¨æ€è·å–æ ¸å¿ƒæ•° ---
    # ä¼˜å…ˆè¯»å– Slurm åˆ†é…çš„æ ¸å¿ƒæ•°ï¼Œå¦‚æœæ²¡è¯»åˆ°ï¼ˆæ¯”å¦‚æœ¬åœ°æµ‹è¯•ï¼‰ï¼Œåˆ™é»˜è®¤ä½¿ç”¨è¾ƒå°‘çš„æ ¸
    slurm_cpus = os.environ.get('SLURM_CPUS_PER_TASK')
    if slurm_cpus:
        num_cores = int(slurm_cpus)
    else:
        # æœ¬åœ°è¿è¡Œæ—¶ä¿ç•™ 2 ä¸ªæ ¸ç»™ç³»ç»Ÿ
        num_cores = max(1, multiprocessing.cpu_count() - 2)

    # è®¡ç®—æœ¬ä»»åŠ¡éœ€è¦ç”Ÿæˆçš„æ•°é‡
    target_range = range(args.start, args.end)
    num_samples_this_job = len(target_range)

    # --- ä¿®æ”¹ 3: è¾“å‡ºè·¯å¾„æ”¹ä¸ºå­æ–‡ä»¶å¤¹ ---
    # æ•°æ®å°†ä¿å­˜åœ¨ data/raw/dataset_v1/parts/ ç›®å½•ä¸‹
    output_dir = "data/raw/dataset_v1/parts"
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸš€ [ä»»åŠ¡ ID {args.part_id}] å¯åŠ¨: å¤„ç†èŒƒå›´ {args.start} -> {args.end} (å…± {num_samples_this_job} ä¸ª)")
    print(f"ğŸ–¥ï¸  è¿è¡ŒèŠ‚ç‚¹: {os.environ.get('SLURMD_NODENAME', 'Localhost')}")
    print(f"ğŸ”¥ ä½¿ç”¨æ ¸å¿ƒæ•°: {num_cores}")

    results = []
    
    # ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œè®¡ç®—
    # æ³¨æ„ï¼šè¿™é‡Œåªå¹¶è¡Œå¤„ç† target_range é‡Œçš„è¿™ä¸€å°éƒ¨åˆ†æ•°æ®
    with multiprocessing.Pool(processes=num_cores) as pool:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ (å¦‚æœæ˜¯ Job Arrayï¼Œæ—¥å¿—é‡Œçš„è¿›åº¦æ¡å¯èƒ½ä¼šæ¯”è¾ƒå¤šï¼Œä½†ä¸å½±å“è¿è¡Œ)
        for res in tqdm(pool.imap_unordered(generate_single_instance, target_range), total=num_samples_this_job):
            if res is not None:
                results.append(res)
    
    # --- ä¿®æ”¹ 4: ä¿å­˜ä¸ºç‹¬ç«‹çš„åˆ†ç‰‡æ–‡ä»¶ ---
    save_path = f"{output_dir}/part_{args.part_id}.npz"
    np.savez_compressed(save_path, data=results)
    
    print(f"\nâœ… [ä»»åŠ¡ ID {args.part_id}] å®Œæˆï¼")
    print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {len(results)}")
    print(f"ğŸ’¾ å·²ä¿å­˜: {save_path}")

if __name__ == "__main__":
    main()