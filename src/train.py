import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
import os
import time
import argparse

# å¼•å…¥æˆ‘ä»¬è‡ªå·±å†™çš„æ¨¡å—
from src.data.dataset import FALQONDataset
from src.models.transformer import FALQONTransformer

def train(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for batch in loader:
        # 1. æ¬è¿æ•°æ®åˆ° GPU
        adj = batch['adj'].to(device)   # [B, N, N]
        mask = batch['mask'].to(device) # [B, N]
        targets = batch['betas'].to(device) # [B, 30]
        
        # 2. å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(adj, mask) # [B, 30]
        
        # 3. è®¡ç®—æŸå¤± & åå‘ä¼ æ’­
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
    return total_loss / len(loader)

def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in loader:
            adj = batch['adj'].to(device)
            mask = batch['mask'].to(device)
            targets = batch['betas'].to(device)
            
            outputs = model(adj, mask)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            
    return total_loss / len(loader)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--data_path", type=str, default="data/raw/dataset_v1/train_data_final.npz")
    parser.add_argument("--save_dir", type=str, default="models/checkpoints")
    args = parser.parse_args()

    # 1. è®¾ç½®è®¾å¤‡ (ä¼˜å…ˆç”¨ GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")

    # 2. å‡†å¤‡æ•°æ®
    full_dataset = FALQONDataset(args.data_path)
    
    # åˆ’åˆ† 80% è®­ç»ƒ, 20% éªŒè¯
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    print(f"ğŸ“Š æ•°æ®é›†å°±ç»ª: è®­ç»ƒé›† {len(train_dataset)} | éªŒè¯é›† {len(val_dataset)}")

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = FALQONTransformer().to(device)
    
    # 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss() # å‡æ–¹è¯¯å·® (Mean Squared Error)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    
    # 5. è®­ç»ƒå¾ªç¯
    os.makedirs(args.save_dir, exist_ok=True)
    best_val_loss = float('inf')
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    for epoch in range(args.epochs):
        train_loss = train(model, train_loader, criterion, optimizer, device)
        val_loss = validate(model, val_loader, criterion, device)
        
        # æ‰“å°è¿›åº¦
        print(f"Epoch [{epoch+1}/{args.epochs}] "
              f"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path = os.path.join(args.save_dir, "best_model.pth")
            torch.save(model.state_dict(), save_path)
            print(f"    ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Loss: {best_val_loss:.6f})")

    total_time = (time.time() - start_time) / 60
    print(f"\nâœ… è®­ç»ƒç»“æŸ! æ€»è€—æ—¶: {total_time:.2f} åˆ†é’Ÿ")
    print(f"ğŸ† æœ€ä½³éªŒè¯é›† Loss: {best_val_loss:.6f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {args.save_dir}")

if __name__ == "__main__":
    main()
