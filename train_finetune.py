import torch
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import networkx as nx
import os
import scipy.linalg

from src.models.spectral_transformer import SpectralTemporalTransformer
from src.physics.simulator import DiffQuantumSimulator

# --- 1. ç®€æ˜“çš„æ•°æ®ç”Ÿæˆå™¨ (åªç”Ÿæˆå›¾ï¼Œä¸éœ€è¦æ ‡ç­¾) ---
class UnlabeledGraphDataset(Dataset):
    def __init__(self, num_samples=500, min_n=10, max_n=14):
        self.data = []
        print(f"Generating {num_samples} unlabeled graphs for Physics Fine-tuning...")
        from tqdm import tqdm
        for _ in tqdm(range(num_samples)):
            n = np.random.randint(min_n, max_n + 1)
            # 50% æ­£åˆ™å›¾ï¼Œ50% ERå›¾
            if np.random.rand() > 0.5 and (n*3)%2==0:
                g = nx.random_regular_graph(3, n)
            else:
                g = nx.erdos_renyi_graph(n, 0.6)
            adj = nx.to_numpy_array(g)
            evals, evecs = self.get_spectral(adj)
            # <--- æ€¥æ•‘ï¼šè¿›ä¸€æ­¥æ”¾å¤§ç‰¹å¾å€¼ä¿¡å·ï¼Œæ‰“ç ´å¹³å‡åŒ–ï¼--->
            evals = evals * 30.0
            # Padding
            evals_pad = np.zeros(20, dtype=np.float32)
            evals_pad[:n] = evals
            evecs_pad = np.zeros((20, 20), dtype=np.float32)
            evecs_pad[:n, :n] = evecs
            adj_pad = np.zeros((20, 20), dtype=np.float32)
            adj_pad[:n, :n] = adj
            self.data.append({
                'evals': torch.from_numpy(evals_pad),
                'evecs': torch.from_numpy(evecs_pad),
                'adj': torch.from_numpy(adj_pad),
                'n': n
            })
    def get_spectral(self, adj):
        deg = np.sum(adj, axis=1)
        d_inv_sqrt = np.power(deg, -0.5, where=deg!=0)
        d_inv_sqrt[deg==0] = 0.0
        D_inv = np.diag(d_inv_sqrt)
        L = np.eye(len(adj)) - D_inv @ adj @ D_inv
        evals, evecs = scipy.linalg.eigh(L)
        return evals.astype(np.float32), evecs.astype(np.float32)
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]

def main():
    # é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = SpectralTemporalTransformer(max_nodes=20, d_model=128, max_seq_len=40).to(device)
    pretrained_path = "models/spectral_transformer_ep100.pth" # ç¡®ä¿è¿™é‡Œè·¯å¾„å¯¹
    
    if os.path.exists(pretrained_path):
        print(f"Loading pretrained model: {pretrained_path}")
        pretrained_dict = torch.load(pretrained_path, map_location=device)
        model_dict = model.state_dict()
        # åªåŠ è½½åŒ¹é…çš„æƒé‡
        matched = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
        model_dict.update(matched)
        model.load_state_dict(model_dict)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(matched)}/{len(model_dict)} ä¸ªé¢„è®­ç»ƒæƒé‡")
        print(f"âš ï¸ æ–°å¢å‚æ•°å°†éšæœºåˆå§‹åŒ–: query_embed, graph_to_query")
    else:
        print("âš ï¸ Warning: No pretrained model found. Training from scratch (Hard!)")
        
    # 2. å‡†å¤‡ç‰©ç†æ¨¡æ‹Ÿå™¨
    # æ³¨æ„: MaxCut æ¨¡æ‹Ÿå™¨çš„ qubit æ•°å¿…é¡»ç­‰äºå›¾èŠ‚ç‚¹æ•°
    # ä¸ºäº† Batch å¹¶è¡Œï¼Œæˆ‘ä»¬æŒ‰èŠ‚ç‚¹æ•°åˆ†ç»„è®­ç»ƒï¼Œæˆ–è€…ç®€å•çš„ï¼š
    # æˆ‘ä»¬çš„æ¨¡æ‹Ÿå™¨æ”¯æŒ batchï¼Œä½†è¦æ±‚ batch å†…çš„ n_qubits ç›¸åŒå—ï¼Ÿ
    # ä¸Šé¢çš„ simulator ä»£ç ä¸­ï¼ŒN æ˜¯å›ºå®šçš„ self.nã€‚
    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸ºä¸åŒçš„ N å®ä¾‹åŒ–ä¸åŒçš„æ¨¡æ‹Ÿå™¨ï¼Œæˆ–è€…åœ¨ä¸€ä¸ª batch é‡Œåªæ”¾ç›¸åŒ N çš„å›¾ã€‚
    # ç­–ç•¥ï¼šæˆ‘ä»¬åªå¾®è°ƒ N=12 çš„å›¾ä½œä¸ºæ¼”ç¤º (Scale up test)
    
    target_N = 12
    simulator = DiffQuantumSimulator(n_qubits=target_N, device=device)
    
    # 3. å‡†å¤‡æ•°æ® (åªç”Ÿæˆ N=12 çš„å›¾)
    dataset = UnlabeledGraphDataset(num_samples=500, min_n=target_N, max_n=target_N)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    optimizer = optim.Adam(model.parameters(), lr=1e-4) # æ€¥æ•‘ï¼šè°ƒå¤§å­¦ä¹ ç‡
    
    # 4. è®­ç»ƒå¾ªç¯
    print("ğŸš€ Starting Physics-Informed Fine-tuning (PALQO Strategy)...")
    for epoch in range(20):
        total_energy = 0
        for batch in loader:
            evals = batch['evals'].to(device)
            evecs = batch['evecs'].to(device)
            adj = batch['adj'].to(device) # [B, 20, 20]
            
            # æˆªå–æœ‰æ•ˆçš„ adj éƒ¨åˆ†ä¼ å…¥æ¨¡æ‹Ÿå™¨ (å› ä¸ºæ¨¡æ‹Ÿå™¨æ˜¯ N=12)
            adj_eff = adj[:, :target_N, :target_N]
            
            # ç”Ÿæˆæ—¶é—´ç´¢å¼•
            time_idx = torch.arange(40, device=device).unsqueeze(0).expand(evals.shape[0], -1)
            
            optimizer.zero_grad()
            
            # (A) å­¦ç”Ÿæ¨¡å‹é¢„æµ‹ Beta
            pred_betas = model(evals, evecs, time_idx) # [B, 40]
            # æ€¥æ•‘ï¼šåŠ å™ªå£°ï¼Œå¸®åŠ©æ¨¡å‹è·³å‡ºæ­»çº¿
            pred_betas = pred_betas + 0.05 * torch.randn_like(pred_betas)
            
            # (B) ç‰©ç†æ¨¡æ‹Ÿå™¨è®¡ç®—èƒ½é‡ (Physics Loss)
            # æ³¨æ„: è¿™é‡Œçš„æ¨¡æ‹Ÿå™¨æ˜¯å¯å¾®çš„ï¼
            energy = simulator.compute_maxcut_energy(pred_betas, adj_eff)
            
            # Loss = Average Energy (æˆ‘ä»¬å¸Œæœ›èƒ½é‡è¶Šä½è¶Šå¥½)
            loss = energy.mean()
            
            loss.backward()
            optimizer.step()
            
            total_energy += loss.item()
            
        avg_energy = total_energy / len(loader)
        print(f"Epoch {epoch+1}: Physics Loss (Energy) = {avg_energy:.4f}")
        
    # ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
    torch.save(model.state_dict(), "models/spectral_transformer_finetuned.pth")
    print("âœ… Fine-tuning complete!")

if __name__ == "__main__":
    main()
