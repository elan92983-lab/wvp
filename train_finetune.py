import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import os
import sys
import argparse

# å¼•å…¥ä½ çš„é¡¹ç›®æ¨¡å—
# å‡è®¾ä½ åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œæˆ–è€…ç¡®ä¿ src å¯è§
from src.models.spectral_transformer import SpectralTemporalTransformer
from src.physics.simulator import DiffQuantumSimulator
from src.data_utils.dataset import SpectralDataset

def temporal_gradient_loss(pred, target, mask):
    """é¼“åŠ±æ¨¡å‹å­¦ä¹  Î² çš„å˜åŒ–è¶‹åŠ¿"""
    pred_diff = pred[:, 1:] - pred[:, :-1]
    target_diff = target[:, 1:] - target[:, :-1]
    mask_diff = mask[:, 1:] * mask[:, :-1]
    return ((pred_diff - target_diff) ** 2 * mask_diff).sum() / (mask_diff.sum() + 1e-6)

def tail_variance_loss(pred, target, mask, tail_ratio=0.5):
    """çº¦æŸååŠæ®µæ–¹å·®ï¼Œé¿å…å°¾éƒ¨å¡Œç¼©æˆç›´çº¿"""
    P = pred.shape[1]
    start = int(P * (1 - tail_ratio))
    tail_mask = mask[:, start:]
    if tail_mask.sum() < 1:
        return pred.sum() * 0.0
    pred_tail = pred[:, start:]
    target_tail = target[:, start:]
    def masked_var(x, m):
        denom = m.sum(dim=1, keepdim=True).clamp_min(1.0)
        mean = (x * m).sum(dim=1, keepdim=True) / denom
        var = ((x - mean) ** 2 * m).sum(dim=1, keepdim=True) / denom
        return var
    pv = masked_var(pred_tail, tail_mask)
    tv = masked_var(target_tail, tail_mask)
    return ((pv - tv) ** 2).mean()

def make_time_weights(seq_len, weight_tail, device):
    if weight_tail <= 1.0:
        return torch.ones(seq_len, device=device)
    return torch.linspace(1.0, weight_tail, steps=seq_len, device=device)


def load_pretrained_safely(model, path, device):
    if not path or not os.path.exists(path):
        print("âš ï¸ é¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")
        return
    state = torch.load(path, map_location=device)
    model_state = model.state_dict()
    filtered = {k: v for k, v in state.items() if k in model_state and v.shape == model_state[k].shape}
    missing = [k for k in model_state.keys() if k not in filtered]
    unexpected = [k for k in state.keys() if k not in filtered]
    model.load_state_dict(filtered, strict=False)
    print(f"âœ… é¢„è®­ç»ƒæƒé‡å·²åŠ è½½ (åŒ¹é… {len(filtered)}/{len(model_state)})ã€‚")
    if missing:
        print(f"âš ï¸ æœªåŠ è½½(å½¢çŠ¶ä¸åŒ¹é…æˆ–ä¸å­˜åœ¨)å‚æ•°æ•°é‡: {len(missing)}")
    if unexpected:
        print(f"âš ï¸ é¢„è®­ç»ƒä¸­å¤šä½™å‚æ•°æ•°é‡: {len(unexpected)}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=80)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--pretrained_path", type=str, default="models/spectral_transformer_ep100.pth")
    parser.add_argument("--weight_mse", type=float, default=5.0)
    parser.add_argument("--weight_temp", type=float, default=0.5)
    parser.add_argument("--weight_tail", type=float, default=2.0)
    parser.add_argument("--ss_start", type=float, default=0.0)
    parser.add_argument("--ss_end", type=float, default=0.5)
    parser.add_argument("--weight_tail_var", type=float, default=0.2)
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # --- 1. é…ç½®è·¯å¾„ ---
    # è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•
    root_dir = os.path.dirname(os.path.abspath(__file__))
    data_path = os.path.join(root_dir, "data/processed/spectral_data_v2.npz")
    
    if not os.path.exists(data_path):
        print(f"âŒ Error: Dataset not found at {data_path}")
        return

    # --- 2. åŠ è½½æ•°æ®é›† (å¸¦æ ‡ç­¾!) ---
    # è¿™æ¬¡æˆ‘ä»¬ç”¨ SpectralDatasetï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦çœŸå®çš„ betas åšè€å¸ˆ
    print("Loading labeled dataset for Supervised + Physics training...")
    dataset = SpectralDataset(data_path, max_nodes=20, max_seq_len=40)
    
    # æ‹†åˆ† Train/Val (å¯é€‰ï¼Œè¿™é‡Œå…¨é‡è®­ç»ƒæ¼”ç¤º)
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    # è·å–ç»Ÿè®¡é‡ (ç”¨äºåå½’ä¸€åŒ–)
    BETA_MEAN = dataset.beta_mean
    BETA_STD = dataset.beta_std
    print(f"Dataset Stats: Mean={BETA_MEAN:.4f}, Std={BETA_STD:.4f}")

    # --- 3. åŠ è½½æ¨¡å‹ ---
    model = SpectralTemporalTransformer(max_nodes=20, d_model=128, max_seq_len=40).to(device)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    load_pretrained_safely(model, args.pretrained_path, device)
    
    # --- 4. ç‰©ç†æ¨¡æ‹Ÿå™¨ & Loss ---
    # æ³¨æ„ï¼šæ¨¡æ‹Ÿå™¨éœ€è¦æ ¹æ® batch å†…æœ€å¤§çš„ N åŠ¨æ€è°ƒæ•´ï¼Œæˆ–è€…å›ºå®šæœ€å¤§å€¼
    # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªæœ€å¤§ N=20 çš„æ¨¡æ‹Ÿå™¨ï¼Œä½†è®¡ç®—æ—¶è¦å°å¿ƒ masking
    # å®é™…ä¸Š DiffQuantumSimulator ç›®å‰æ˜¯å›ºå®š N çš„ã€‚
    # ä¸ºäº†è§£å†³å˜é•¿å›¾çš„ç‰©ç† Lossï¼Œæˆ‘ä»¬è¿™é‡Œåªå¯¹ batch é‡Œ N <= 12 çš„å›¾è®¡ç®—ç‰©ç† Loss (ä½œä¸ºè¾…åŠ©)
    # æˆ–è€…ï¼šä¸»è¦é  MSEï¼Œç‰©ç† Loss ä»…ä½œä¸º "éªŒè¯" æˆ– "å¾®å¼±æ­£åˆ™"
    
    target_N_sim = 12
    simulator = DiffQuantumSimulator(n_qubits=target_N_sim, device=device)
    
    mse_criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr) # å­¦ä¹ ç‡

    # === è°ƒå‚æ ¸å¿ƒåŒºåŸŸ ===
    # å¦‚æœä½ æƒ³è®©æ›²çº¿å¾ˆåƒ FALQONï¼šè°ƒå¤§ WEIGHT_MSE (æ¯”å¦‚ 1.0, 10.0)
    # å¦‚æœä½ æƒ³è®©èƒ½é‡å¾ˆé«˜ï¼šè°ƒå¤§ WEIGHT_PHYS (æ¯”å¦‚ 0.1, 0.5)
    # å»ºè®®ï¼šå…ˆç”¨ MSE å¼ºè¡Œæ•™ä¼šå½¢çŠ¶ï¼Œå†å¾®è°ƒ
    WEIGHT_MSE = args.weight_mse      # <--- è°ƒå¤§è¿™ä¸ªï¼å¼ºè¿«å®ƒæ¨¡ä»¿
    WEIGHT_TEMP = args.weight_temp    # <--- æ—¶é—´æ¢¯åº¦è¶‹åŠ¿ loss
    WEIGHT_PHYS = 0.05                # <--- ç‰©ç†åªåšè¾…åŠ©ï¼Œå› ä¸ºæ¨¡æ‹Ÿå™¨å¯¹å˜é•¿å›¾æ”¯æŒæœ‰é™
    WEIGHT_TAIL = args.weight_tail
    WEIGHT_TAIL_VAR = args.weight_tail_var
    
    print("ğŸš€ Starting Supervised + Physics Fine-tuning...")
    model.train()
    
    for epoch in range(args.epochs): # å¤šè·‘å‡ ä¸ª epoch
        total_loss = 0
        total_mse = 0
        total_phys = 0
        
        for batch in loader:
            evals = batch['evals'].to(device)
            evecs = batch['evecs'].to(device)
            time_idx = batch['time_indices'].to(device)
            mask = batch['mask'].to(device)     # [B, P]
            real_betas = batch['betas'].to(device) # [B, P] (Normalized)
            num_nodes = batch['num_nodes'].to(device)      # [B]
            
            optimizer.zero_grad()
            
            # (A) Scheduled Sampling
            ss_prob = args.ss_start + (args.ss_end - args.ss_start) * (epoch / max(args.epochs - 1, 1))
            ss_prob = float(max(0.0, min(1.0, ss_prob)))

            prev_tf = torch.zeros_like(real_betas)
            prev_tf[:, 1:] = real_betas[:, :-1]

            with torch.no_grad():
                pred_tf = model(evals, evecs, time_idx, num_nodes=num_nodes, prev_betas=prev_tf)

            ss_mask = torch.rand_like(real_betas[:, 1:]) < ss_prob
            prev_betas = prev_tf.clone()
            prev_betas[:, 1:] = torch.where(ss_mask, pred_tf[:, :-1], real_betas[:, :-1])

            pred_betas = model(evals, evecs, time_idx, num_nodes=num_nodes, prev_betas=prev_betas) # [B, P] (Normalized)
            
            # (B) Loss 1: MSE (æ¨¡ä»¿è€å¸ˆ)
            # åªè®¡ç®— mask=1 çš„éƒ¨åˆ† (æœ‰æ•ˆæ—¶é—´æ­¥)
            loss_mse = (pred_betas - real_betas) ** 2
            time_w = make_time_weights(loss_mse.shape[1], WEIGHT_TAIL, loss_mse.device).unsqueeze(0)
            weighted_mask = mask * time_w
            loss_mse = (loss_mse * weighted_mask).sum() / weighted_mask.sum().clamp_min(1.0)
            
            # (C) Loss 2: Physics (è¾…åŠ©)
            # åªæŒ‘é€‰ batch ä¸­ N == target_N_sim çš„æ ·æœ¬è®¡ç®—ç‰©ç† Loss
            # å¦‚æœ batch é‡Œæ²¡æœ‰ N=12 çš„ï¼Œå°±è·³è¿‡ç‰©ç† Loss
            loss_phys = torch.tensor(0.0, device=device)
            
            # ç­›é€‰ç¬¦åˆæ¨¡æ‹Ÿå™¨å¤§å°çš„å›¾
            indices = torch.nonzero(num_nodes == target_N_sim, as_tuple=False).squeeze(-1)
            if indices.numel() > 0 and WEIGHT_PHYS > 0:
                # æå–å­é›†
                sub_betas_norm = pred_betas.index_select(0, indices)
                # åå½’ä¸€åŒ–ä¾›æ¨¡æ‹Ÿå™¨ä½¿ç”¨
                sub_betas_phys = sub_betas_norm * BETA_STD + BETA_MEAN
                
                # æ­¤æ—¶ Dataset å¹¶æ²¡æœ‰è¿”å› adj (SpectralDataset é»˜è®¤ä¸ºäº†çœå†…å­˜æ²¡å­˜ adj)
                # è¿™æ˜¯ä¸€ä¸ªå°é—®é¢˜ã€‚å¦‚æœ dataset.py æ²¡è¿”å› adjï¼Œæˆ‘ä»¬æ— æ³•è®¡ç®—ç‰©ç† lossã€‚
                # === ç´§æ€¥ç­–ç•¥ ===
                # å¦‚æœ Dataset é‡Œæ²¡ adjï¼Œæˆ‘ä»¬å°±åªç”¨ MSEï¼
                # æ—¢ç„¶ä½ è¦ "æ¥è¿‘å®é™…æ›²çº¿"ï¼ŒMSE æ‰æ˜¯ 99% é‡è¦çš„ã€‚
                pass 
                
            # (D) Total Loss
            loss_temp = temporal_gradient_loss(pred_betas, real_betas, mask)
            loss_tail_var = tail_variance_loss(pred_betas, real_betas, mask, tail_ratio=0.5)
            loss = WEIGHT_MSE * loss_mse + WEIGHT_TEMP * loss_temp + WEIGHT_TAIL_VAR * loss_tail_var
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_mse += loss_mse.item()
            
        print(f"Epoch {epoch+1}: Total Loss={total_loss/len(loader):.4f} | MSE={total_mse/len(loader):.4f}")

    # ä¿å­˜
    torch.save(model.state_dict(), "models/spectral_transformer_finetuned.pth")
    print("âœ… Fine-tuning complete! Model learned from teacher (MSE).")

if __name__ == "__main__":
    main()